information gain
Entropy是接收的所有訊息中所包含的資訊的平均量，也可以解釋entropy是看資料的亂度或是資料的不確定性
完全猜不到的情況，Entropy=1。

cross-entropy
https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9-%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8-loss-function-2dcac5ebb6cb


Huber loss是為了改善均方誤差損失函數(Squared loss function)對outlier的穩健性(robustness)而提出的
δ是Huber loss的參數。
第一眼看Huber loss都會覺得很複雜，但其實它就是squared loss和 absolute loss的合成。


但focal loss是希望針對inliers(easy example)進行down-weighting，因為 focal loss希望的是在訓練過程中能盡量去訓練hard example，忽略那些easy example。
原因在於focal loss是提出來在物件偵測用，但往往一張圖片找出的候選物件或是Anchor有絕大部分的比例是背景(background)而不是前景(foreground，也就是物件)，所以在計算loss會有極大的不平衡(extreme imbalance)的問題


VIF(variance inflation factor) of ˆβj : v_All/ v_modelSelf
variance
standard error: 反映樣本平均數對總體平均數的變異程度，從而反映抽樣誤差的大小，是量度結果精密度的指標。
標準差(亦稱單數標準差)一般用s表示，是表示個體間變異大小的指標，反映了整個樣本對樣本平均數的離散程度，是數據精密度的衡量指標。
隨著樣本數(或測量次數)n的增大，標準差趨向某個穩定值，即樣本標準差s越接近總體標準差σ，而標準誤則隨著樣本數(或測量次數)n的增大逐漸減小，即樣本平均數越接近總體平均數μ；故在實驗中也經常採用適當增加樣本數(或測量次數)n減小s_{\bar{x}}的方法來減小實驗誤差，但樣本數太大意義也不大。標準差是最常用的統計量，一般用於表示一組樣本變數的分散程度；標準誤一般用於統計推斷中，主要包括假設檢驗和參數估計，如樣本平均數的假設檢驗、參數的區間估計與點估計等。
t-statistic
z-statistic
f-statistic
p-value
confidence interval
prediction interval


the z-statistic associated with β1 is equal to ˆ β1/SE( ˆ β1), and so a
large (absolute) value of the z-statistic indicates evidence against the null
hypothesis H0 : β1 = 0.

LDA(Linear Discriminant Analysis):
Assumption: the observations
within each class come from a normal distribution with a class-specific
mean vector and a common(or class-specific) variance σ2, and plugging estimates for these
parameters into the Bayes classifier.
Bayes posterior probabiliy -log-> discriminant function = linear func of  x.

can logistic be polynomial? Yes, sure. So as LDA -> 形狀類似 QDA

Bootstrap: repeatly sampling observation with replacement. (1/3 的資料不被訓練到，而可以作為測試集)
如果用来做模型的选择和预测误差的估计，CV的特点是bias小，方差大，方差基本不会因为fold数目提高有明显减小，Booststrap的特点是bias大但是方差小。bias描述的是根据样本拟合出的模型的输出预测结果的期望与样本真实结果的差距，简单讲，就是在样本上拟合的好不好。
CV主要用于model selection，例如KNN中选多大的K，使得估计的test error比较小。而Bootstrap主要用来看选定的model的uncertainty，例如参数的标准差多大。
cv -> one-standard-error rule: find the smallest model within lowest point +- one starndard error of estimated MSE

best subset selection (2**p)
forward/ backward/ mixed selection (stepwise selection) p(p+1)/2

Cp: estimate of test MSE by (RSS+2dσ**2)/n, if ˆσ2 is an unbiased estimate of σ2 in (6.2), then Cp is an unbiased estimate of test MSE.
Akaike information criterion (AIC): is defined for a large class of models fit by maximum likelihood.
Bayesian information criterion
adjusted r2: penalize number of features(n-p-1/n-1)

principal component: Zi
principal component score: raw data 轉換到z向量上的值
Partial Least Squares(PLS): supervised PCR using idea like boosting 

Regression: 
classifer: logistic, LDA, QDA
bias, variance

caveat
reiterate
discriminant
jargon

